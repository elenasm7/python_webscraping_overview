{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping in Python for Beginners\n",
    "\n",
    "In this notebook, I will give an introduction to webscraping in python. This will not be an exhaustive overview of all the resources at your disposal with python, but it should be enough to help you in the beginning. The library I will be going over in this post is Beautiful Soup (utilizing Requests). In my next tutorial, I will go over how to utalize Beautiful Soup and Selenium together. The only other major option you have available is Scrapy. Along the way, I will discuss your options and leave more tutorials for different aspects of this project that are outside of the scope of Beautiful Soup.\n",
    "\n",
    "#### First we need to import the needed libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup _Only_:\n",
    "\n",
    "Beautiful Soup is a python package that parces XML and HTML. It makes it very easy to get data from the html data avaible on the website. However, it does have it's pitfalls. These include not being able to grab data as easily from dynamic pages. In those situations we will use Selenium.\n",
    "\n",
    "So, in the cell below we can start with a quick example. Let's try with a few pages to get a feel for how things change. So, below lets define three variables (url_1, url_2, and url_3) to three different urls.\n",
    "\n",
    "The first is Zara's dresses page, flatiron data science page, and pelton's yelp page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_1 = 'https://www.zara.com/us/en/woman-dresses-l1066.html?v1=1180427'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first we need to use the Requests library to request the url and assign that to the variable page. We can then use page in the beautiful soup function, and pass 'lxml' as the second argument. This just means that Beautiful Soup is parcing the page as lxml.\n",
    "\n",
    "By convention we will save the results of BeautifulSoup as soup (in this case FI_soup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zara_page = requests.get(url_1)\n",
    "zara_soup = BeautifulSoup(zara_page.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can already view this, simply run the cell below to view the page html: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "zara_soup #this may time out due to the page being too big\n",
    "\n",
    "zara_soup.find('div') #this will return the first div on the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we have the html saved as a variable we can parce through the page to get the information we need.\n",
    "\n",
    "In the pervious cell we saw one of the most common ways to grab information using Beautiful Soup. The two methods __find__ and __findAll__ will be your best friends in this process. However, there are many other ways to grab infomation from the html if these do not work. And we will discuss those next.\n",
    "\n",
    "If we go and inspect our page, we can see the classnames and IDs of items that we want. below is the screen shot of what the it looked like when I inspected each product item. In this exercise, we will be grabbing the products name and price, and saving it as a dictionary.\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/inspect_zara.png\"  width=800>\n",
    "\n",
    "if you look at it we can easily see the classname for the divs surrounding the product name and the price, **'name _item'** and **'price _product_price'** respectively.\n",
    "\n",
    "#### Lets first see if we can grab all of the product names.\n",
    "\n",
    "In Beautiful Soup the method findAll allows us to do just that. If we use the find method, it will only retun the first itme that it finds that meets the requirements passed, while finaAll returns every item that meets the requirements.\n",
    "\n",
    "As seen above, the first thing we pass in the method is the html tag (i.e. 'div','p','h1','a') and the second will be a dictionary with a key value pair. Typically you will see **'class'** with the actual class name or **'ID'**. We will be using class.\n",
    "\n",
    "After grabbing all of the links with this class name _'name'_ we will use a [list comprehension](https://hackernoon.com/list-comprehension-in-python-8895a785550b) to create a list of the names. If you are not familiar with list comprehensions, I highly recommend them! \n",
    "\n",
    "After, lets view the first 5 elements of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DRESS WITH DRAPED NECKLINE',\n",
       " 'POLKA DOT DRESS',\n",
       " 'LINEN DRESS WITH BUTTONS',\n",
       " 'FRINGED TEXTURED WEAVE TUNIC',\n",
       " 'RUFFLED DRESS']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = zara_soup.findAll('a',{'class','name'})\n",
    "product_names = [item.text for item in names]\n",
    "product_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part for prices gets a little complicated. Due to how the html is parced we need to use a little regex to grab the prices! So first, what we need to do is to grab the divs with the prices and lets look at the first item in the list before we move forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"price _product-price\"><span data-price=\"49.90 USD\"></span></div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices = zara_soup.findAll('div',{'class','_product-price'})\n",
    "prices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very frequently we can just use the dot method text, which would return the text in the divs. However, usually if the text is surrounded by a span we are unable to do this. If we test it below we can see that this returns an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"WOMEN'S DRESSES FOR EVERY MOMENT\"\n"
     ]
    }
   ],
   "source": [
    "# zara_soup.h1.descendants\n",
    "for string in zara_soup.h1.children:\n",
    "    print(repr(string))\n",
    "#     print(i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DRESS WITH DRAPED NECKLINE'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zara_soup.find('a',{'class','name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some Regex. We will use Python's 'Re' library's search method as well as it's group. If we look at all of the prices we realize they are surrounded in a span and have a special html tag called 'data-price'. So to grab this information pass the first chuck of html before the price <img style=\"display: inline-block;\" src=\"images/span_img.png\"  width=150> and before each special character we will use a '\\' to break and let the regex parcer know that we need that special charater in our search. next we will have parentheses with '.*' in it which just means any length/combination of characters, and then finish it with ' USD\"><' this is how our sting should end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49.9, 69.9, 69.9, 49.9, 49.9]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_prices = [float(re.search('\\<span data\\-price\\=\\\"(.*)\\ USD\\\"\\>\\<',str(item)).group(1)) for item in prices]\n",
    "prod_prices[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next - Selenium:\n",
    "\n",
    "#### The majority of your projects will consist of the previous steps.\n",
    "\n",
    "However, as i mentioned before there are other ways to grab data other than using the find and findAll dot methods for html tags. [This](https://selenium-python.readthedocs.io/locating-elements.html) is a great resource to hopefully give you some information on your different options, however I will outline them here.\n",
    "\n",
    "Each of the following methods to locate items on the page follow the same two patterns. To **locate the first element** on the page that matches we do: **\".find\\_element\\_by\\_[name of item we want to find]\"**. If we want to **find all of the items** and return them as a list we do: **\".find\\_elements\\_by\\_[name of item we want to find]\"**. The items that can be used here are:\n",
    "- id\n",
    "- name\n",
    "- xpath\n",
    "- link_text\n",
    "- partial_link_text\n",
    "- tag_name\n",
    "- class_name\n",
    "- css_selector\n",
    "\n",
    "These can replace the findAll and find methods from before, however I tend to keep the those methods and do not use id, name, tag_name, and class_name. The methods that have been most useful are xpath, link_text, and partial_link_text. I will walk through these three below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab elements using their _siblings_\n",
    "\n",
    "previously we discussed grabbing the next element in a list, or the next item that would be captured if you did the findAll method for a specific, or not so specific tag. Using the two methods .next_sibling and .previous_sibling can be very useful. This is a Beautiful Soup method, but I believe it is worth the mention.\n",
    "\n",
    "Here is a quick example on how to implement both. First you need to select an item on the page. Then select the HTML tag you're looking to jump in between (i.e. \".a\" | \".div\"), and then follow it with the attribute \".next_sibling\", shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a aria-expanded=\"false\" aria-label=\"TRF, Show other categories\" class=\"_category-link \" data-extraquery=\"v1=437642\" href=\"https://www.zara.com/us/en/trf-l880.html\" style=\"color: #F53993;\" tabindex=\"0\"><span class=\"cat-name\">TRF</span></a>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the chrome driver\n",
    "zara_page = requests.get(url_1)\n",
    "zara_soup = BeautifulSoup(zara_page.content, 'lxml')\n",
    "\n",
    "#selected item:\n",
    "selected = zara_soup.find('li',{'class':'_category-link-wrapper'})\n",
    "\n",
    "#grab next item:\n",
    "selected.li.next_sibling\n",
    "\n",
    "#to get the link to that sibling:\n",
    "selected.li.next_sibling.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .find\\_element\\_by\\_xpath & .find\\_elements\\_by\\_xpath\n",
    "\n",
    "Xpath can be very useful. I have used it most when divs and other html elements do not provide a class_name or a class_name is the same as the other items. Sometimes, you only want to select the second item of a specific class and instead of grabbing all of them you can also use this.\n",
    "\n",
    "There are other options to do the latter task and that will be briefly desribed next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for mac64 chromedriver:75.0.3770.90 in cache\n",
      "Driver found in /Users/elenasm7/.wdm/chromedriver/75.0.3770.90/mac64/chromedriver\n"
     ]
    }
   ],
   "source": [
    "#create the chrome driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install()) \n",
    "driver.get(url_1)\n",
    "time.sleep(0.5)\n",
    "driver.find_element_by_xpath(\"//a[@aria-label='WOMAN, Show other categories']\").click()\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .find\\_element\\_by\\_link_text & .find\\_elements\\_by\\_link_text\n",
    "\n",
    "I will only go over by\\_link\\_text. These two methods are essentially the same. The biggest reason to use partial\\_link\\_text over \\_link\\_text is a simple one. Many times parts of a link or item on the page will remain the same, so if this is the case for you and you want to grab all of the items with similar text, then you would use only partial. However, if you have a very specific item you need then you would use all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for mac64 chromedriver:75.0.3770.90 in cache\n",
      "Driver found in /Users/elenasm7/.wdm/chromedriver/75.0.3770.90/mac64/chromedriver\n"
     ]
    }
   ],
   "source": [
    "#create the chrome driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install()) \n",
    "driver.get(url_1)\n",
    "time.sleep(0.5)\n",
    "driver.find_element_by_link_text('WOMAN').click()\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Selenium to get the right data:\n",
    "\n",
    "One issue I've run into multiple times is having many items on the page that are not what I need. So, you can filter out your results and then use this new url. But what if the url doesn't change? What if the data that you need can only be cleaned up by directly filtering you results while you're scraping? This is where Selenium comes in. Zara is a good example of this, but happens on many pages. \n",
    "\n",
    "Below lets look into this and how to filter the page properly. If you haven't already imported Selenium and the webdriver manager do this now. The ChromeDriver manager is good if you do not know where your chromedriver was installed. However, after you run this once, it will print where it found your driver--for whichever one you have. If you need more information on how to install selenium and the webdriver look [here for ChromeDriver](http://chromedriver.chromium.org/) and here for [WebDriver_Manager](https://github.com/SergeyPirogov/webdriver_manager). \n",
    "\n",
    "Lets create start our webdriver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for mac64 chromedriver:75.0.3770.90 in cache\n",
      "Driver found in /Users/elenasm7/.wdm/chromedriver/75.0.3770.90/mac64/chromedriver\n"
     ]
    }
   ],
   "source": [
    "#create the chrome driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install()) \n",
    "driver.get(url_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you run the WebDriverManager for the first time you will get an output that looks like: **Driver found in /Users/YOUR_USER_NAME/.wdm/chromedriver/75.0.3770.90/mac64/chromedriver**. You will then be able to put that into the webdriver.Chrome() parenthesis instead of the manager itself!\n",
    "\n",
    "Now you should have seen a new chrome window open up to the url you specified. In my case, it'll be Zara's dresses page. Now, we need to inspect the filters to determine what we need to select:\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/zara_filter_inspect.png\"  width=600>\n",
    "\n",
    "So, after inspecting the filter button, we can see that the class we need to be selecting by is: **'products-filter'**. Lets do this below:\n",
    "\n",
    "_**NOTE**: At this point it may be tempting to select by partial link text, but this is not a linked item, even if it seems like it is._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now, filter the page by the types of bags we want to see\n",
    "driver.find_element_by_class_name('products-filter').click()\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important: you need to add sleep to your webdriver, and to any scraping scripts. It is not only beneficial to you, but also to the website owner. It will decrease your odds of being kicked off.__\n",
    "\n",
    "You will now see the filter menu open up in your driver window. Next lets look into the different filters and select the items we need. The first thing to note is that each filter category is long enough that it is truncated to fit on the screen. So, first we will need to expand all of those. This is what it looks like when you inspect it:\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/filter_inspect.png\"  width=600>\n",
    "\n",
    "In this case we will find all the elements with the class name **'\\_view-more-button'**, and then click them. You can accomplish this by creating a list of these objects and looping through the as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_sections = driver.find_elements_by_class_name('_view-more-button')\n",
    "for item in filter_sections:\n",
    "    time.sleep(0.5)\n",
    "    item.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that all of our lists are expanded we can find out how to select each of the necessary filters. For simplicity's sake, we will only be selecting two separate filters. Lets check out how we can best grab these filters:\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/filters.png\"  width=600>\n",
    "\n",
    "If we inspect this further, the only thing that distingusishes each filter from each other, other than the actual text, is this attribute called data-value. For this example we see a size small is: __data-value=\"7000000000000000014\"__. Becuase of this, we will be using the xpath method to select these filter items. Below we will select the style __Embroidered__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath(\"//div[@data-value='7000000000000711017']\").click()\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most websites you will be able to use Beautiful Soup after this step, and simply repeat the steps from the first tutorial. However, in this one we will contine using Selenium. However, if you were to use this with Beautiful Soup, all you would need to do is grab the page source by using the .page_source method, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create an html item for beautiful soup\n",
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets grab all of the items with the class name product. After I will show you another great Selenium method to get all the hmtl inside of an element. The method is .get_attribute(), which you then pass 'innerHTML'.\n",
    "\n",
    "This is much harder than simply using BS4 to find this infomation on the page, which is why its preferred to use Beautiful Soup.\n",
    "\n",
    "Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dresses = driver.find_elements_by_class_name('product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting this, I realzied that all items from the page were still there. However, one major thing did change: _**class name**_. If you inspect it, you wil notice that all items that were filtered out had the class \"\\_hidden\". So, now we will filter these out by using a for loop, and if statement, and the method .get_attribute() and passing that the keyword class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_html_li = []\n",
    "for dress in dresses:\n",
    "    time.sleep(0.5)\n",
    "    if '_hidden' not in dress.get_attribute('class'):\n",
    "        product_html_li.append(dress.get_attribute('innerHTML'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the length to make sure we do not have more items than needed, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_html_li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I have created a few for loops to use regex to clean up the innerHTML we are accessing. If you look into what we grabbed, it is a mess. However, we can overcome this by using regex to select the items within it. Google is your best friend when writing out your regex if your new to it. It is complicated and at the end of this tutorial I will include some resources that have helped me along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "product_info_dict = {'product_url':[],\n",
    "                    'product_name':[],\n",
    "                    'product_price':[]}\n",
    "\n",
    "#loop through the lists and append the items\n",
    "for i in product_html_li:\n",
    "    url = re.search('.*?href\\=\\\"(.*)\\\"\\ data\\-extraquery\\=\\\".*\\\"\\>\\<div class\\=\\\"product-grid-xmedia.*', i)\n",
    "    name = re.search('.*?alt\\=\\\"(.*)\\\" src\\=\\\".*', i)\n",
    "    price = re.search('.*\\ data\\-price\\=\\\"(.*)\\\"\\>.*', i)\n",
    "    product_info_dict['product_price'].append(float(price.group(1).replace('USD','')))\n",
    "    product_info_dict['product_name'].append(name.group(1))\n",
    "    product_info_dict['product_url'].append(url.group(1))\n",
    "\n",
    "end_time = time.time()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the finished dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product_name': ['EMBROIDERED MESH DRESS',\n",
       "  'EMBROIDERED TUNIC WITH POCKET',\n",
       "  'DRESS WITH CROCHET TRIM',\n",
       "  'CROCHETED DRESS',\n",
       "  'EMBROIDERED DRESS',\n",
       "  'COLORFUL EMBROIDERED TUNIC'],\n",
       " 'product_price': [22.99, 49.99, 45.99, 35.99, 22.99, 45.99],\n",
       " 'product_url': ['https://www.zara.com/us/en/embroidered-mesh-dress-p05598152.html',\n",
       "  'https://www.zara.com/us/en/embroidered-tunic-with-pocket-p04786072.html',\n",
       "  'https://www.zara.com/us/en/dress-with-crochet-trim-p00881111.html',\n",
       "  'https://www.zara.com/us/en/crocheted-dress-p02712318.html',\n",
       "  'https://www.zara.com/us/en/embroidered-dress-p01131316.html',\n",
       "  'https://www.zara.com/us/en/colorful-embroidered-tunic-p00881210.html']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Great, it looks perfect! However, typically I would say to use the skills shown in my last tutorial on Beautiful Soup after accessing/filtering your page using Selenium. Sometimes this is not the case and you'll need to use it for both parts of your project. If that's the case, I hope this gave enough clairty on how you can best apply it. For another example on how to use it, check out my jupyter notebook for [my capstone project](https://github.com/elenasm7/NYC_fitness_recommender/blob/master/Mod_5_functions.py). My notebooks are open and well documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
