{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping in Python for Beginners\n",
    "\n",
    "In this notebook, I will give an introduction to webscraping in python. This will not be an exhaustive overview of all the resources at your disposal with python, but it should be enough to help you in the beginning. The library I will be going over in this post is Beautiful Soup (utilizing Requests). In my next tutorial, I will go over how to utalize Beautiful Soup and Selenium together. The only other major option you have available is Scrapy. Along the way, I will discuss your options and leave more tutorials for different aspects of this project that are outside of the scope of Beautiful Soup.\n",
    "\n",
    "#### First we need to import the needed libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup _Only_:\n",
    "\n",
    "Beautiful Soup is a python package that parces XML and HTML. It makes it very easy to get data from the html data avaible on the website. However, it does have it's pitfalls. These include not being able to grab data as easily from dynamic pages. In those situations we will use Selenium.\n",
    "\n",
    "So, in the cell below we can start with a quick example. Let's try with a few pages to get a feel for how things change. So, below lets define three variables (url_1, url_2, and url_3) to three different urls.\n",
    "\n",
    "The first is Zara's dresses page, flatiron data science page, and pelton's yelp page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_1 = 'https://www.zara.com/us/en/woman-dresses-l1066.html?v1=1180427'\n",
    "url_2 = 'https://flatironschool.com/career-courses/data-science-bootcamp/'\n",
    "url_3 = 'https://www.yelp.com/biz/peloton-new-york?osq=peloton'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first we need to use the Requests library to request the url and assign that to the variable page. We can then use page in the beautiful soup function, and pass 'lxml' as the second argument. This just means that Beautiful Soup is parcing the page as lxml.\n",
    "\n",
    "By convention we will save the results of BeautifulSoup as soup (in this case FI_soup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zara_page = requests.get(url_1)\n",
    "zara_soup = BeautifulSoup(zara_page.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can already view this, simply run the cell below to view the page html: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"_global-loader active\" id=\"global-loader\"></div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zara_soup #this may time out due to the page being too big\n",
    "\n",
    "zara_soup.find('div') #this will return the first div on the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we have the html saved as a variable we can parce through the page to get the information we need.\n",
    "\n",
    "In the pervious cell we saw one of the most common ways to grab information using Beautiful Soup. The two methods __find__ and __findAll__ will be your best friends in this process. However, there are many other ways to grab infomation from the html if these do not work. And we will discuss those next.\n",
    "\n",
    "If we go and inspect our page, we can see the classnames and IDs of items that we want. below is the screen shot of what the it looked like when I inspected each product item. In this exercise, we will be grabbing the products name and price, and saving it as a dictionary.\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/inspect_zara.png\"  width=800>\n",
    "\n",
    "if you look at it we can easily see the classname for the divs surrounding the product name and the price, **'name _item'** and **'price _product_price'** respectively.\n",
    "\n",
    "#### Lets first see if we can grab all of the product names.\n",
    "\n",
    "In Beautiful Soup the method findAll allows us to do just that. If we use the find method, it will only retun the first itme that it finds that meets the requirements passed, while finaAll returns every item that meets the requirements.\n",
    "\n",
    "As seen above, the first thing we pass in the method is the html tag (i.e. 'div','p','h1','a') and the second will be a dictionary with a key value pair. Typically you will see **'class'** with the actual class name or **'ID'**. We will be using class.\n",
    "\n",
    "After grabbing all of the links with this class name _'name'_ we will use a [list comprehension](https://hackernoon.com/list-comprehension-in-python-8895a785550b) to create a list of the names. If you are not familiar with list comprehensions, I highly recommend them! \n",
    "\n",
    "After, lets view the first 5 elements of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DRESS WITH DRAPED NECKLINE',\n",
       " 'POLKA DOT DRESS',\n",
       " 'LINEN DRESS WITH BUTTONS',\n",
       " 'FRINGED TEXTURED WEAVE TUNIC',\n",
       " 'RUFFLED DRESS']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = zara_soup.findAll('a',{'class','name'})\n",
    "product_names = [item.text for item in names]\n",
    "product_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part for prices gets a little complicated. Due to how the html is parced we need to use a little regex to grab the prices! So first, what we need to do is to grab the divs with the prices and lets look at the first item in the list before we move forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"price _product-price\"><span data-price=\"49.90 USD\"></span></div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices = zara_soup.findAll('div',{'class','_product-price'})\n",
    "prices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very frequently we can just use the dot method text, which would return the text in the divs. However, usually if the text is surrounded by a span we are unable to do this. If we test it below we can see that this returns an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some Regex. We will use Python's 'Re' library's search method as well as it's group. If we look at all of the prices we realize they are surrounded in a span and have a special html tag called 'data-price'. So to grab this information pass the first chuck of html before the price <img style=\"display: inline-block;\" src=\"images/span_img.png\"  width=150> and before each special character we will use a '\\' to break and let the regex parcer know that we need that special charater in our search. next we will have parentheses with '.*' in it which just means any length/combination of characters, and then finish it with ' USD\"><' this is how our sting should end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49.9, 69.9, 69.9, 49.9, 49.9]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_prices = [float(re.search('\\<span data\\-price\\=\\\"(.*)\\ USD\\\"\\>\\<',str(item)).group(1)) for item in prices]\n",
    "prod_prices[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The majority of your projects will consist of these steps.\n",
    "\n",
    "However, as i mentioned before there are other ways to grab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
