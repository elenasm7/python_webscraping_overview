{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping in Python for Beginners \n",
    "\n",
    "In this notebook, I will give an introduction to webscraping in python. This will not be an exhaustive overview of all the resources at your disposal with python, but it should be enough to help you in the beginning. The two tools I will be going over in this post are Beautiful Soup (utilizing Requests) and Selenium. The only other major option you have available is Scrapy. First, we will go over Beautiful Soup, then Selenium, and in the end, we will bring them together! Along the way, I will discuss the pros and cons of both.\n",
    "\n",
    "#### First we need to import the needed libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup _Only_:\n",
    "\n",
    "Beautiful Soup is a python package that parces XML and HTML. It makes it very easy to get data from the html data avaible on the website. However, it does have it's pitfalls. These include not being able to grab data as easily from dynamic pages. In those situations we will use Selenium.\n",
    "\n",
    "So, in the cell below we can start with a quick example. Let's try with a few pages to get a feel for how things change. So, below lets define three variables (url_1, url_2, and url_3) to three different urls.\n",
    "\n",
    "The first is Zara's dresses page, flatiron data science page, and pelton's yelp page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_1 = 'https://www.zara.com/us/en/woman-dresses-l1066.html?v1=1180427'\n",
    "url_2 = 'https://flatironschool.com/career-courses/data-science-bootcamp/'\n",
    "url_3 = 'https://www.yelp.com/biz/peloton-new-york?osq=peloton'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first we need to use the Requests library to request the url and assign that to the variable page. We can then use page in the beautiful soup function, and pass 'lxml' as the second argument. This just means that Beautiful Soup is parcing the page as lxml.\n",
    "\n",
    "By convention we will save the results of BeautifulSoup as soup (in this case FI_soup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zara_page = requests.get(url_1)\n",
    "zara_soup = BeautifulSoup(zara_page.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can already view this, simply run the cell below to view the page html: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"_global-loader active\" id=\"global-loader\"></div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zara_soup #this may time out due to the page being too big\n",
    "\n",
    "zara_soup.find('div') #this will return the first div on the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we have the html saved as a variable we can parce through the page to get the information we need.\n",
    "\n",
    "If we go and inspect our page, we can see the classnames and IDs of items that we want. below is the screen shot of what the it looked like when I inspected each product item. In this exercise, we will be grabbing the products name and price, and saving it as a dictionary.\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/inspect_zara.png\"  width=800>\n",
    "\n",
    "if you look at it we can easily see the classname for the divs surrounding the product name and the price, **'name _item'** and **'price _product_price'** respectivery.\n",
    "\n",
    "#### Lets first see if we can grab all of the product names very quickly.\n",
    "\n",
    "In Beautiful Soup the method findAll allows us to do just that. If we use the find method, it will only retun the first itme that it finds that meets the requirements passed. \n",
    "\n",
    "As seen above, the first thing we pass in the method is the html tag (i.e. 'div','p','h1','a') and the second will be a dictionary with a key value pair. Typically you will see 'class' with the actual class name or 'ID'. We will be using class.\n",
    "\n",
    "After grabbing all of the links with this class name _'name'_ we will use a [list comprehension](https://hackernoon.com/list-comprehension-in-python-8895a785550b) to create a list of the names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FLORAL PRINT BALLOON SLEEVE DRESS',\n",
       " 'FLORAL PRINT DRESS',\n",
       " 'LONG LINEN DRESS',\n",
       " 'PLEATED DRESS',\n",
       " 'FLORAL PRINT DRESS']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = zara_soup.findAll('a',{'class','name'})\n",
    "product_names = [item.text for item in names]\n",
    "product_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part for prices gets a little complicated. Due to how the html is parced we need to use a little regex to grab the prices! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = zara_soup.findAll('div',{'class','_product-price'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some Regex. We will use Python's 'Re' library's search method as well as it's group. If we look at all of the prices we realize they are surrounded in a span and have a special html tag called 'data-price'. So to grab this information pass the first chuck of html before the price <img style=\"display: inline-block;\" src=\"images/span_img.png\"  width=150> and before each special character we will use a '\\' to break and let the regex parcer know that we need that special charater in our search. next we will have parentheses with '.*' in it which just means any length/combination of characters, and then finish it with ' USD\"><' this is how our sting should end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[69.9, 49.9, 89.9, 69.9, 49.9]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_prices = [float(re.search('\\<span data\\-price\\=\\\"(.*)\\ USD\\\"\\>\\<',str(item)).group(1)) for item in prices]\n",
    "prod_prices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
